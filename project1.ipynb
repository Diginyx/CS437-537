{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading File and creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "wiki_files = pd.read_csv('wiki_sample.csv')\n",
    "wiki_dataframe = pd.DataFrame(wiki_files)\n",
    "wiki_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for entry in wiki_dataframe['tokenized_content']:\n",
    "    sum += len(entry)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words\n",
    "len(inv_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing and Creating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import pycountry\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from names_dataset import NameDatasetV1 # v1\n",
    "names = NameDatasetV1()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# for domain_stop_word in domain_stop_words:\n",
    "#     stop_words.add(domain_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "companies_file = pd.read_csv('companies_sorted.csv')\n",
    "companies_dataframe = pd.DataFrame(companies_file)\n",
    "companies_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = set(companies_dataframe['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "lowerCasedWords = map(lambda word: word.lower(), nltk.corpus.words.words())\n",
    "lowerCasedWords = set(list(lowerCasedWords))\n",
    "for country in list(pycountry.countries):\n",
    "    lowerCasedWords.add(country.name.lower())\n",
    "for company in companies:\n",
    "    lowerCasedWords.add(str(company).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization, lowercase, remove non alphanumeric, remove non-english, remove numbers and stopword removal\n",
    "rejected_content = []\n",
    "def preprocess_vocab(row, lenTitle):\n",
    "    filtered_content = []\n",
    "    for token in nltk.word_tokenize(row['content'][lenTitle:]):\n",
    "        token = lemmatizer.lemmatize(token).lower()\n",
    "        if names.search_first_name(token) or names.search_last_name(token) or ((token in lowerCasedWords) and (token not in stop_words) and (token.isalpha())):\n",
    "            filtered_content.append(token) \n",
    "        else:\n",
    "            rejected_content.append(token)\n",
    "        \n",
    "    return filtered_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataframe['tokenized_content'] = wiki_dataframe.progress_apply(lambda row: preprocess_vocab(row, len(row['title'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataframe.to_pickle('./wiki_dataframe_augmented_nltk_corpus_to_remove_non-english.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "with open(\"./data/wiki_dataframe_augmented_nltk_corpus_to_remove_non-english.pkl\", \"rb\") as pickle_file:\n",
    "    wiki_dataframe = pickle.load(pickle_file)\n",
    "# wiki_dataframe = pd.read_pickle('./wiki_dataframe_augmented_nltk_corpus_to_remove_non-english.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Part of Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "inv_idx = defaultdict(list)\n",
    "vocab = set()\n",
    "heaps_law_dataset = list()\n",
    "\n",
    "most_freq = []\n",
    "id = 1\n",
    "total_words = 0\n",
    "for document in tqdm(wiki_dataframe['tokenized_content']):\n",
    "    counter = Counter(document)\n",
    "    most_occur = counter.most_common(1)\n",
    "    most_freq.append(most_occur)\n",
    "    heaps_law_dataset.append((total_words, len(vocab)))\n",
    "    for word in document:\n",
    "        inv_idx[word].append(id)\n",
    "        total_words += 1\n",
    "        vocab.add(word)\n",
    "    id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataframe['most_frequent_term'] = most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "inv_idx_ordered = OrderedDict(sorted(inv_idx.items(), key=lambda item: len(item[1]), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(inv_idx_ordered.keys())\n",
    "domain_stop_words = words[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heaps Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heaps_law_dataset[len(heaps_law_dataset)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def heaps_law(list_to_graph):\n",
    "        x = list()\n",
    "        y = list()\n",
    "        \n",
    "        for item in list_to_graph:\n",
    "            x.append(item[0])\n",
    "            y.append(item[1])\n",
    "\n",
    "        plt.plot(x, y)\n",
    "        plt.xlim(1, x[-1])\n",
    "        plt.ylim(1, y[-1])\n",
    "        plt.savefig(\"heaps_law_words_from_nltk_english_corpus.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heaps_law(heaps_law_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipfs Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def zipfs_law(list_to_graph):\n",
    "        x = list()\n",
    "        y = list()\n",
    "        \n",
    "        for i, word in enumerate(list_to_graph):\n",
    "            x.append(i+1)\n",
    "            y.append(len(inv_idx_ordered[word]))\n",
    "            \n",
    "\n",
    "        plt.loglog(x, y)\n",
    "        plt.savefig(\"zipfs_law.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfs_law(inv_idx_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in tqdm(inv_idx_ordered.items()):\n",
    "    inv_idx[value[0]] = (Counter(value[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_write = open(\"inv_idx_augmented_nltk_corpus_to_remove_non-english.pkl\", \"wb\")\n",
    "pickle.dump(inv_idx, file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx = pickle.load(open(\"inv_idx_augmented_nltk_corpus_to_remove_non-english.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggesting Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "aol_query_log = pd.read_csv('project_1_AOL_query_log/Clean-Data-01.txt', sep=\"\\t\")\n",
    "for file in listdir('project_1_AOL_query_log')[1:]:\n",
    "    aol_query_log = aol_query_log.append(pd.read_csv('project_1_AOL_query_log/' + str(file), sep=\"\\t\"), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_scores = []\n",
    "# for candidate in candidates:\n",
    "#     num_sessions_q_modified = 0\n",
    "#     for ID in set(aol_query_log[aol_query_log['Query'] == original_query]['AnonID']):\n",
    "#         candidate_list = list(aol_query_log[aol_query_log['AnonID'] == ID]['Query'])\n",
    "#         try:\n",
    "#             index = candidate_list.index(original_query)\n",
    "#         except: \n",
    "#             continue\n",
    "#         if candidate[1] in candidate_list[index+1:]:\n",
    "#             num_sessions_q_modified += 1\n",
    "#         candidate_scorse.append((candidate, num_sessions_q_modified / num_session_containing_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(aol_query_log[aol_query_log['Query'] == 'vietnam']['AnonID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aol_query_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(aol_query_log['AnonID'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization, lowercase, remove non alphanumeric, and stopword removal\n",
    "def query_logs_preprocessing(row):\n",
    "    filtered_content = \"\"\n",
    "    for token in nltk.word_tokenize(str(row['Query'])):\n",
    "        token = lemmatizer.lemmatize(token).lower()\n",
    "        if token not in stop_words and token.isalpha():\n",
    "            filtered_content += token + \" \"\n",
    "    return filtered_content[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aol_query_log['Processed Query'] = aol_query_log.progress_apply(lambda row: query_logs_preprocessing(row), axis=1)\n",
    "aol_query_log.to_pickle('./data/aol_query_log_data.pkl')\n",
    "aol_query_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aol_queries = aol_query_log['Processed Query'].values\n",
    "def identify_candidate_queries(query):\n",
    "    candidate_queries = set()\n",
    "    for aol_query in aol_queries:\n",
    "        if len(aol_query.split()) > len(query.split()):\n",
    "            if aol_query.startswith(query):\n",
    "                candidate_queries.add(aol_query)\n",
    "    return candidate_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Ranking Candidate Suggestions\n",
    "##### ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ¶ğ‘„, ğ‘â€²) = \\# ğ‘œğ‘“ ğ‘ ğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›ğ‘  ğ‘–ğ‘› ğ‘¤â„ğ‘–ğ‘â„ ğ‘ ğ‘–ğ‘  ğ‘šğ‘œğ‘‘ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ ğ‘¡ğ‘œ ğ¶ğ‘„ Ã· \\# ğ‘œğ‘“ ğ‘ ğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›ğ‘  ğ‘–ğ‘› ğ‘¤â„ğ‘–ğ‘â„ ğ‘ ğ‘ğ‘ğ‘ğ‘’ğ‘ğ‘Ÿğ‘ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(aol_query_log[aol_query_log['AnonID'] == 217]['Query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aol_queries = aol_query_log['Query'].values\n",
    "# n^3 algorithm ask dr. pera if this is valid algorithm\n",
    "def OLD_rank_candidate_queries(original_query, candidates):\n",
    "    candidate_scores = []\n",
    "    num_session_containing_q = 0\n",
    "    num_sessions_q_modified = 0\n",
    "    \n",
    "    sessions_with_original_query = set(aol_query_log[aol_query_log['Query'] == original_query]['AnonID'])\n",
    "    num_session_containing_q = len(sessions_with_original_query)\n",
    "            \n",
    "    for candidate in tqdm(candidates):\n",
    "        num_sessions_q_modified = 0\n",
    "        for ID in sessions_with_original_query:\n",
    "            candidate_list = list(aol_query_log[aol_query_log['AnonID'] == ID]['Query'])\n",
    "            try:\n",
    "                index = candidate_list.index(original_query)\n",
    "            except: \n",
    "                continue\n",
    "            if candidate[1] in candidate_list[index+1:]:\n",
    "                num_sessions_q_modified += 1\n",
    "        candidate_scores.append((candidate, num_sessions_q_modified / num_session_containing_q))\n",
    "    return sorted(candidate_scores, key=lambda candidate_score: candidate_score[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidate_queries(original_query, candidates):\n",
    "    candidate_scores = []\n",
    "    num_sessions_containing_q = 0    \n",
    "    sessions_with_original_query = set(aol_query_log[aol_query_log['Query'] == original_query]['AnonID'])\n",
    "    # print(sessions_with_original_query)\n",
    "    # return sessions_with_original_query \n",
    "    num_sessions_containing_q = len(sessions_with_original_query)\n",
    "    \n",
    "    for candidate in tqdm(candidates): #cut off at certain number of candidates?\n",
    "        sessions_with_candidate_query = set(aol_query_log[aol_query_log['Query'] == candidate]['AnonID'])\n",
    "        sessions_with_both_queries = sessions_with_original_query & sessions_with_candidate_query\n",
    "        if num_sessions_containing_q != 0:\n",
    "            candidate_scores.append((candidate, len(sessions_with_both_queries) / num_sessions_containing_q))\n",
    "        \n",
    "    for candidate in candidates: # supplementing the final list if the ranked results dont equal five \n",
    "        if len(candidate_scores) >= 5:\n",
    "            break\n",
    "        candidate_scores.append((candidate, 0)) # candidate score of 0 is default rank\n",
    "    return sorted(candidate_scores, key=lambda candidate_score: candidate_score[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rank_candidate_queries(query):\n",
    "    candidate_list = identify_candidate_queries(query)\n",
    "    return rank_candidate_queries(query, candidate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_rank_candidate_queries(\"frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Identifying Candidate Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def identify_candidate_resources(query):\n",
    "    results = set()\n",
    "    split_query = query.split()\n",
    "    n = len(split_query)\n",
    "    candidate_list = list()\n",
    "    for term in split_query:\n",
    "        if len(inv_idx[term]) > 0:\n",
    "            candidate_list.append(set(inv_idx[term].keys()))\n",
    "    if len(candidate_list) > 0:\n",
    "        results = set.intersection(*candidate_list)\n",
    "    if len(results) <= 50:\n",
    "        for combination in combinations(split_query, n - 1):\n",
    "            candidate_list = list()\n",
    "            for term in combination:\n",
    "                  candidate_list.append(inv_idx[term])\n",
    "            if len(candidate_list) > 0:\n",
    "                results = set.intersection(*candidate_list)\n",
    "            if len(results) > 50:\n",
    "                break\n",
    "            else:\n",
    "                n -= 1\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_candidate_resources('sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>most_frequent_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182211</th>\n",
       "      <td>Les Eyzies-de-Tayac-Sireuil\\r\\n\\r\\nLes Eyzies-...</td>\n",
       "      <td>Les Eyzies de Tayac Sireuil</td>\n",
       "      <td>182212</td>\n",
       "      <td>[les, commune, department, southwestern, franc...</td>\n",
       "      <td>[(les, 4)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content  \\\n",
       "182211  Les Eyzies-de-Tayac-Sireuil\\r\\n\\r\\nLes Eyzies-...   \n",
       "\n",
       "                              title      id  \\\n",
       "182211  Les Eyzies de Tayac Sireuil  182212   \n",
       "\n",
       "                                        tokenized_content most_frequent_term  \n",
       "182211  [les, commune, department, southwestern, franc...         [(les, 4)]  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TF-IDF\n",
    "#### ğ‘‡ğ¹(ğ‘¤, ğ‘‘) = ğ‘“ğ‘Ÿğ‘’ğ‘(ğ‘¤, ğ‘‘) Ã· (ğ‘šğ‘ğ‘¥_ğ‘‘)\n",
    "#### ğ¼ğ·ğ¹(ğ‘¤) = ğ‘™ğ‘œğ‘”__2 (ğ‘ Ã· ğ‘›_ğ‘¤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(wiki_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def tf_idf(split_query, document_id):\n",
    "    score = 0.001 # not zero for normalization\n",
    "    \n",
    "    if document_id == 0:\n",
    "        print(\"error in tf-idf function\")\n",
    "        return\n",
    "        \n",
    "    for term in split_query:\n",
    "        if term not in inv_idx:\n",
    "            continue\n",
    "        score += (inv_idx[term][document_id] / wiki_dataframe['most_frequent_term'][document_id - 1][0][1]) * math.log((len(wiki_dataframe) / len(inv_idx[term])), 2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf([\"word\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx[\"frozen\"][53]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking candidate resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidate_resources(query, candidate_resources):\n",
    "  ranked_candidates = {}\n",
    "  for document_id in candidate_resources:\n",
    "    ranked_candidates[document_id] = tf_idf(query.split(), document_id)\n",
    "  return sorted(ranked_candidates.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_rank_candidate_resources(query):\n",
    "    candidates = identify_candidate_resources(query)\n",
    "    return rank_candidate_resources(query, candidates)\n",
    "relevant_resources = find_and_rank_candidate_resources(\"sex\")[0]\n",
    "relevant_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippet(query, document_id):\n",
    "    snippet = tuple() # in the form (title, sentences)\n",
    "    row = wiki_dataframe[wiki_dataframe['id'] == document_id + 1]\n",
    "    snippet = (row[\"title\"].to_string(index=False), generate_sentence_snippets(query, document_id, int(row[\"title\"].str.len()))) \n",
    "    return snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Orel Anzio',\n",
       " ('The Orel Anzio was a Minor League Baseball club that played in the Italian Baseball League in its 2006 season.',\n",
       "  'The team was based in Anzio, a city located on the coast of the Lazio region of Italy, about 33 miles south of Rome which is noted as an important historical port.'))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_snippet(\"sex\",915)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_sentence_snippets(query, document_id, len_title):\n",
    "    pattern = \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"\n",
    "    top_two = [] # format is (sentence, cosine_similarity_score)\n",
    "    vectorized_query = vectorize(query, document_id)\n",
    "    for sentence in re.split(pattern, wiki_dataframe[\"content\"][document_id][len_title:]):\n",
    "        sentence = sentence.rstrip()[4:] # 4 is for removal of \\r\\n\\r\\n for each sentence.\n",
    "        lsentence = sentence.lower()\n",
    "        \n",
    "        sentence_score = cosine_similarity(vectorized_query, vectorize(lsentence, document_id))\n",
    "        if len(top_two) < 2:\n",
    "            top_two.append((sentence, sentence_score))\n",
    "            top_two.sort(key=lambda item: item[1], reverse=True)\n",
    "            continue\n",
    "        \n",
    "        for index, entry in enumerate(top_two): # this loop should only run twice\n",
    "            if sentence_score > entry[1]: \n",
    "                top_two.insert(index, (sentence, sentence_score))\n",
    "                continue\n",
    "    return (top_two[0][0], top_two[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(phrase, document_id):\n",
    "    arr = []\n",
    "    for word in phrase.split():\n",
    "        arr.append(tf_idf([word], document_id))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def cosine_similarity(vectorized_query, vectorized_sentence):\n",
    "    lenf = max(len(vectorized_query), len(vectorized_sentence))\n",
    "    for i in range (lenf + 1):\n",
    "        if len(vectorized_query) == len(vectorized_sentence):\n",
    "            break\n",
    "        if (len(vectorized_query) < i):\n",
    "            vectorized_query.append(0.001)\n",
    "        if (len(vectorized_sentence) < i):\n",
    "            vectorized_sentence.append(0.001)\n",
    "    return (1 - spatial.distance.cosine(list(vectorized_sentence), list(vectorized_query)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa55fa09e0e4422978c3061674d30740611d5f2a5ecbc5414dc2c8743e27cab3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
