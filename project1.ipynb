{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\jared\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading File and creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "wiki_files = pd.read_csv('wiki_sample.csv')\n",
    "wiki_dataframe = pd.DataFrame(wiki_files)\n",
    "wiki_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for entry in wiki_dataframe['tokenized_content']:\n",
    "    sum += len(entry)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words\n",
    "len(inv_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing and Creating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import pycountry\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from names_dataset import NameDatasetV1 # v1\n",
    "names = NameDatasetV1()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# for domain_stop_word in domain_stop_words:\n",
    "#     stop_words.add(domain_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "companies_file = pd.read_csv('companies_sorted.csv')\n",
    "companies_dataframe = pd.DataFrame(companies_file)\n",
    "companies_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = set(companies_dataframe['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "lowerCasedWords = map(lambda word: word.lower(), nltk.corpus.words.words())\n",
    "lowerCasedWords = set(list(lowerCasedWords))\n",
    "for country in list(pycountry.countries):\n",
    "    lowerCasedWords.add(country.name.lower())\n",
    "for company in companies:\n",
    "    lowerCasedWords.add(str(company).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization, lowercase, remove non alphanumeric, remove non-english, remove numbers and stopword removal\n",
    "rejected_content = []\n",
    "def preprocess_vocab(row, lenTitle):\n",
    "    filtered_content = []\n",
    "    for token in nltk.word_tokenize(row['content'][lenTitle:]):\n",
    "        token = lemmatizer.lemmatize(token).lower()\n",
    "        if names.search_first_name(token) or names.search_last_name(token) or ((token in lowerCasedWords) and (token not in stop_words) and (token.isalpha())):\n",
    "            filtered_content.append(token) \n",
    "        else:\n",
    "            rejected_content.append(token)\n",
    "        \n",
    "    return filtered_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataframe['tokenized_content'] = wiki_dataframe.progress_apply(lambda row: preprocess_vocab(row, len(row['title'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataframe.to_pickle('./wiki_dataframe_augmented_nltk_corpus_to_remove_non-english.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "with open(\"./data/wiki_dataframe_augmented_nltk_corpus_to_remove_non-english.pkl\", \"rb\") as pickle_file:\n",
    "    wiki_dataframe = pickle.load(pickle_file)\n",
    "# wiki_dataframe = pd.read_pickle('./wiki_dataframe_augmented_nltk_corpus_to_remove_non-english.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Part of Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "inv_idx = defaultdict(list)\n",
    "vocab = set()\n",
    "heaps_law_dataset = list()\n",
    "\n",
    "most_freq = []\n",
    "id = 1\n",
    "total_words = 0\n",
    "for document in tqdm(wiki_dataframe['tokenized_content']):\n",
    "    counter = Counter(document)\n",
    "    most_occur = counter.most_common(1)\n",
    "    most_freq.append(most_occur)\n",
    "    heaps_law_dataset.append((total_words, len(vocab)))\n",
    "    for word in document:\n",
    "        inv_idx[word].append(id)\n",
    "        total_words += 1\n",
    "        vocab.add(word)\n",
    "    id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataframe['most_frequent_term'] = most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "inv_idx_ordered = OrderedDict(sorted(inv_idx.items(), key=lambda item: len(item[1]), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(inv_idx_ordered.keys())\n",
    "domain_stop_words = words[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heaps Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heaps_law_dataset[len(heaps_law_dataset)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def heaps_law(list_to_graph):\n",
    "        x = list()\n",
    "        y = list()\n",
    "        \n",
    "        for item in list_to_graph:\n",
    "            x.append(item[0])\n",
    "            y.append(item[1])\n",
    "\n",
    "        plt.plot(x, y)\n",
    "        plt.xlim(1, x[-1])\n",
    "        plt.ylim(1, y[-1])\n",
    "        plt.savefig(\"heaps_law_words_from_nltk_english_corpus.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heaps_law(heaps_law_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipfs Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def zipfs_law(list_to_graph):\n",
    "        x = list()\n",
    "        y = list()\n",
    "        \n",
    "        for i, word in enumerate(list_to_graph):\n",
    "            x.append(i+1)\n",
    "            y.append(len(inv_idx_ordered[word]))\n",
    "            \n",
    "\n",
    "        plt.loglog(x, y)\n",
    "        plt.savefig(\"zipfs_law.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfs_law(inv_idx_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in tqdm(inv_idx_ordered.items()):\n",
    "    inv_idx[value[0]] = (Counter(value[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_write = open(\"inv_idx_augmented_nltk_corpus_to_remove_non-english.pkl\", \"wb\")\n",
    "pickle.dump(inv_idx, file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx = pickle.load(open(\"inv_idx_augmented_nltk_corpus_to_remove_non-english.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggesting Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "aol_query_log = pd.read_csv('project_1_AOL_query_log/Clean-Data-01.txt', sep=\"\\t\")\n",
    "for file in listdir('project_1_AOL_query_log')[1:]:\n",
    "    aol_query_log = aol_query_log.append(pd.read_csv('project_1_AOL_query_log/' + str(file), sep=\"\\t\"), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_scores = []\n",
    "# for candidate in candidates:\n",
    "#     num_sessions_q_modified = 0\n",
    "#     for ID in set(aol_query_log[aol_query_log['Query'] == original_query]['AnonID']):\n",
    "#         candidate_list = list(aol_query_log[aol_query_log['AnonID'] == ID]['Query'])\n",
    "#         try:\n",
    "#             index = candidate_list.index(original_query)\n",
    "#         except: \n",
    "#             continue\n",
    "#         if candidate[1] in candidate_list[index+1:]:\n",
    "#             num_sessions_q_modified += 1\n",
    "#         candidate_scorse.append((candidate, num_sessions_q_modified / num_session_containing_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(aol_query_log[aol_query_log['Query'] == 'vietnam']['AnonID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aol_query_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(aol_query_log['AnonID'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying Candidate Queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84919it [00:00, 1124619.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "aol_queries = aol_query_log['Query'].values\n",
    "def identify_candidate_queries(query):\n",
    "    print(\"Identifying Candidate Queries...\")\n",
    "    candidate_queries = defaultdict(int)\n",
    "    potential = list()\n",
    "    sessions_with_original_query = set(aol_query_log[aol_query_log['Query'] == query]['AnonID'])\n",
    "    for anon_id in sessions_with_original_query:\n",
    "        potential_queries = aol_query_log[aol_query_log['AnonID'] == anon_id]['Query']\n",
    "        if len(potential_queries) > 1:\n",
    "            potential.append(potential_queries)\n",
    "    for aol_query in itertools.chain(*potential):\n",
    "        if len(str(aol_query).split()) > len(query.split()):\n",
    "            if aol_query.startswith(query):\n",
    "                candidate_queries[aol_query] += 1\n",
    "    print(\"Done!\")\n",
    "    return candidate_queries, len(sessions_with_original_query)\n",
    "candidate_queries, num_sessions_containing_q = identify_candidate_queries(\"men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidate_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Ranking Candidate Suggestions\n",
    "##### ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ¶ğ‘„, ğ‘â€²) = \\# ğ‘œğ‘“ ğ‘ ğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›ğ‘  ğ‘–ğ‘› ğ‘¤â„ğ‘–ğ‘â„ ğ‘ ğ‘–ğ‘  ğ‘šğ‘œğ‘‘ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ ğ‘¡ğ‘œ ğ¶ğ‘„ Ã· \\# ğ‘œğ‘“ ğ‘ ğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›ğ‘  ğ‘–ğ‘› ğ‘¤â„ğ‘–ğ‘â„ ğ‘ ğ‘ğ‘ğ‘ğ‘’ğ‘ğ‘Ÿğ‘ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Candidate Queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('men bodies', 0.18072289156626506),\n",
       " (\"men's health\", 0.08433734939759036),\n",
       " ('mens penis', 0.07228915662650602),\n",
       " ('mental illness', 0.07228915662650602),\n",
       " ('menstrual cycle', 0.07228915662650602),\n",
       " ('men penis', 0.060240963855421686),\n",
       " ('mens wedding rings', 0.060240963855421686),\n",
       " (\"men's robe\", 0.060240963855421686),\n",
       " ('men fucking', 0.04819277108433735),\n",
       " ('mens health', 0.03614457831325301),\n",
       " ('men and penis', 0.03614457831325301),\n",
       " ('menstruating women', 0.03614457831325301),\n",
       " ('men having sex with animals', 0.03614457831325301),\n",
       " ('mens leather bags', 0.03614457831325301),\n",
       " ('mens rings', 0.03614457831325301),\n",
       " ('meniscus posterior root', 0.03614457831325301),\n",
       " ('mens zip up heavy hooded sweats', 0.03614457831325301),\n",
       " ('mens gold necklace', 0.03614457831325301),\n",
       " ('men cruising men in pa', 0.03614457831325301),\n",
       " ('men and women', 0.024096385542168676),\n",
       " ('mens nude body', 0.024096385542168676),\n",
       " ('mens wacky hairstyles', 0.024096385542168676),\n",
       " (\"men's leather wallet\", 0.024096385542168676),\n",
       " ('men wedding rings', 0.024096385542168676),\n",
       " ('men and dating', 0.024096385542168676),\n",
       " ('mens girdle', 0.024096385542168676),\n",
       " ('mens health our biggest arms workout ever', 0.024096385542168676),\n",
       " ('men and farm animals', 0.024096385542168676),\n",
       " ('men dicks', 0.024096385542168676),\n",
       " ('men nude', 0.024096385542168676),\n",
       " ('mens waxing', 0.024096385542168676),\n",
       " ('men waxing', 0.024096385542168676),\n",
       " (\"men's getaways\", 0.012048192771084338),\n",
       " (\"men's private clubs\", 0.012048192771084338),\n",
       " (\"men's shirt collars\", 0.012048192771084338),\n",
       " ('men collars and cuffs', 0.012048192771084338),\n",
       " ('men in panties', 0.012048192771084338),\n",
       " ('men seduce men', 0.012048192771084338),\n",
       " ('men and women making love', 0.012048192771084338),\n",
       " ('men and women having sex', 0.012048192771084338),\n",
       " ('mens body', 0.012048192771084338),\n",
       " (\"men's dress suits\", 0.012048192771084338),\n",
       " (\"men's dress hats\", 0.012048192771084338),\n",
       " ('men body parts', 0.012048192771084338),\n",
       " ('men sex bodies', 0.012048192771084338),\n",
       " ('mens overlarge penis', 0.012048192771084338),\n",
       " ('mens abnormal penis', 0.012048192771084338),\n",
       " ('mens big penis', 0.012048192771084338),\n",
       " ('men going commando', 0.012048192771084338),\n",
       " ('men commando', 0.012048192771084338),\n",
       " ('menu for passover', 0.012048192771084338),\n",
       " ('menus for diabetics', 0.012048192771084338),\n",
       " ('men kiss men', 0.012048192771084338),\n",
       " ('mental health and cutting out caffeine', 0.012048192771084338),\n",
       " (\"men's blue lace prom shirts\", 0.012048192771084338),\n",
       " (\"men's prom rentals\", 0.012048192771084338),\n",
       " ('mens sunglasses', 0.012048192771084338),\n",
       " ('menopause the musical milwaukee', 0.012048192771084338),\n",
       " ('mental hospitals', 0.012048192771084338),\n",
       " ('mens haircut', 0.012048192771084338),\n",
       " ('men and there feelings', 0.012048192771084338),\n",
       " ('men sweating', 0.012048192771084338),\n",
       " ('men clammy', 0.012048192771084338),\n",
       " ('mens accessories box', 0.012048192771084338),\n",
       " ('mentor silicone case study', 0.012048192771084338),\n",
       " ('mens eyebrows', 0.012048192771084338),\n",
       " ('menu ladies lunch', 0.012048192771084338),\n",
       " ('menstrual cups', 0.012048192771084338),\n",
       " ('men sucking cock', 0.012048192771084338),\n",
       " ('men forcing women to have sex', 0.012048192771084338),\n",
       " ('mens wholesale shoes', 0.012048192771084338),\n",
       " ('menage trois', 0.012048192771084338),\n",
       " ('mens sport coats', 0.012048192771084338),\n",
       " ('mens warehouse', 0.012048192771084338),\n",
       " (\"men's heath\", 0.012048192771084338),\n",
       " ('men nylon designer wallets', 0.012048192771084338),\n",
       " (\"men's leather trifold wallet\", 0.012048192771084338),\n",
       " ('men at work', 0.012048192771084338),\n",
       " ('mens underwear', 0.012048192771084338),\n",
       " ('mens rugby massachusetts', 0.012048192771084338),\n",
       " ('mens shorts', 0.012048192771084338),\n",
       " ('menstruation after childbirth', 0.012048192771084338),\n",
       " ('men and eating disorders', 0.012048192771084338),\n",
       " ('mens gold necklaces', 0.012048192771084338),\n",
       " ('mens gold necklaces list', 0.012048192771084338),\n",
       " (\"menu for fox's pizza\", 0.012048192771084338),\n",
       " ('menstrual cycle calculator', 0.012048192771084338),\n",
       " ('mentally disabled and osteoporosis', 0.012048192771084338),\n",
       " ('mens fitness magazine', 0.012048192771084338),\n",
       " ('men sex videos', 0.012048192771084338),\n",
       " ('men and sheep', 0.012048192771084338),\n",
       " ('menstrual pain', 0.012048192771084338),\n",
       " ('mens eyewear fashion', 0.012048192771084338),\n",
       " ('mens eyewear', 0.012048192771084338),\n",
       " ('mens tatoos', 0.012048192771084338),\n",
       " ('mens tattoos', 0.012048192771084338),\n",
       " ('men having sex', 0.012048192771084338),\n",
       " ('men loves to go down', 0.012048192771084338),\n",
       " ('men wanting to fuck men now', 0.012048192771084338),\n",
       " ('men sex', 0.012048192771084338),\n",
       " ('men with no shirts', 0.012048192771084338),\n",
       " ('men eating cock', 0.012048192771084338),\n",
       " ('men with tongue pierced', 0.012048192771084338),\n",
       " ('men burberry swimwear', 0.012048192771084338)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rank_candidate_queries(original_query, candidates, num_sessions_containing_q):\n",
    "    print(\"Ranking Candidate Queries\")\n",
    "    candidate_scores = []\n",
    "\n",
    "    for candidate in tqdm(candidates): \n",
    "        if num_sessions_containing_q != 0:\n",
    "            candidate_scores.append((candidate, candidates[candidate] / num_sessions_containing_q))\n",
    "   \n",
    "    if len(candidate_scores) < 1:\n",
    "        candidate_scores.append((original_query, 0))\n",
    "    print(\"Done!\")\n",
    "    return sorted(candidate_scores, key=lambda candidate_score: candidate_score[1], reverse=True)\n",
    "rank_candidate_queries(\"men\", candidate_queries, num_sessions_containing_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rank_candidate_queries(query):\n",
    "    candidate_list, num_sessions_containing_q = identify_candidate_queries(query)\n",
    "    return rank_candidate_queries(query, candidate_list, num_sessions_containing_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_rank_candidate_queries(\"frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Identifying Candidate Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def identify_candidate_resources(query):\n",
    "    print(\"Identifying Candidate Resources...\")\n",
    "    results = set()\n",
    "    split_query = query.split()\n",
    "    n = len(split_query)\n",
    "    candidate_list = list()\n",
    "    for term in split_query:\n",
    "        print(\".\", end=\"\")\n",
    "        if len(inv_idx[term]) > 0:\n",
    "            candidate_list.append(set(inv_idx[term].keys()))\n",
    "    if len(candidate_list) > 0:\n",
    "        results = set.intersection(*candidate_list)\n",
    "    if len(results) <= 50:\n",
    "        for combination in combinations(split_query, n - 1):\n",
    "            print(\".\", end=\"\")\n",
    "            candidate_list = list()\n",
    "            for term in combination:\n",
    "                candidate_list.append(inv_idx[term])\n",
    "            if len(candidate_list) > 0:\n",
    "                results = set.intersection(*candidate_list)\n",
    "            if len(results) > 50:\n",
    "                break\n",
    "            else:\n",
    "                n -= 1\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{229377,\n",
       " 327681,\n",
       " 278529,\n",
       " 507907,\n",
       " 655365,\n",
       " 737284,\n",
       " 131080,\n",
       " 720904,\n",
       " 737288,\n",
       " 442384,\n",
       " 131095,\n",
       " 229399,\n",
       " 131097,\n",
       " 901151,\n",
       " 1179680,\n",
       " 1277987,\n",
       " 524325,\n",
       " 426023,\n",
       " 360490,\n",
       " 1163306,\n",
       " 458796,\n",
       " 1163309,\n",
       " 1163310,\n",
       " 639024,\n",
       " 589873,\n",
       " 524338,\n",
       " 442417,\n",
       " 639026,\n",
       " 426043,\n",
       " 1163325,\n",
       " 442430,\n",
       " 999486,\n",
       " 458817,\n",
       " 1097795,\n",
       " 1032262,\n",
       " 1130568,\n",
       " 81993,\n",
       " 557130,\n",
       " 655436,\n",
       " 196685,\n",
       " 196687,\n",
       " 229456,\n",
       " 655442,\n",
       " 1146962,\n",
       " 114770,\n",
       " 278611,\n",
       " 639059,\n",
       " 770135,\n",
       " 1130584,\n",
       " 1081434,\n",
       " 245851,\n",
       " 32860,\n",
       " 1163358,\n",
       " 852065,\n",
       " 1130594,\n",
       " 737379,\n",
       " 671846,\n",
       " 639083,\n",
       " 573548,\n",
       " 933995,\n",
       " 933998,\n",
       " 442482,\n",
       " 737394,\n",
       " 98420,\n",
       " 999539,\n",
       " 1065078,\n",
       " 999543,\n",
       " 1032311,\n",
       " 491642,\n",
       " 163963,\n",
       " 721020,\n",
       " 131197,\n",
       " 32894,\n",
       " 999547,\n",
       " 999549,\n",
       " 999550,\n",
       " 999551,\n",
       " 98435,\n",
       " 1097857,\n",
       " 180356,\n",
       " 1048710,\n",
       " 196743,\n",
       " 999562,\n",
       " 163979,\n",
       " 1032335,\n",
       " 180369,\n",
       " 49301,\n",
       " 704664,\n",
       " 213145,\n",
       " 835741,\n",
       " 1147039,\n",
       " 1147042,\n",
       " 508066,\n",
       " 737443,\n",
       " 65705,\n",
       " 327849,\n",
       " 950443,\n",
       " 180394,\n",
       " 966826,\n",
       " 737453,\n",
       " 508082,\n",
       " 1065141,\n",
       " 245942,\n",
       " 295095,\n",
       " 1097911,\n",
       " 901307,\n",
       " 786620,\n",
       " 442557,\n",
       " 295103,\n",
       " 573634,\n",
       " 393413,\n",
       " 622791,\n",
       " 1048777,\n",
       " 1114314,\n",
       " 229579,\n",
       " 671949,\n",
       " 803023,\n",
       " 213200,\n",
       " 49361,\n",
       " 868565,\n",
       " 803031,\n",
       " 196824,\n",
       " 196826,\n",
       " 16602,\n",
       " 540891,\n",
       " 721119,\n",
       " 1163487,\n",
       " 377057,\n",
       " 196835,\n",
       " 229605,\n",
       " 65766,\n",
       " 311526,\n",
       " 1130726,\n",
       " 65769,\n",
       " 753897,\n",
       " 409832,\n",
       " 131310,\n",
       " 1229038,\n",
       " 737519,\n",
       " 721137,\n",
       " 213233,\n",
       " 360691,\n",
       " 917748,\n",
       " 540914,\n",
       " 737525,\n",
       " 901368,\n",
       " 1097976,\n",
       " 540922,\n",
       " 65787,\n",
       " 393468,\n",
       " 655611,\n",
       " 1016062,\n",
       " 1196287,\n",
       " 131329,\n",
       " 426241,\n",
       " 721154,\n",
       " 934145,\n",
       " 508162,\n",
       " 999682,\n",
       " 16645,\n",
       " 262411,\n",
       " 950540,\n",
       " 164109,\n",
       " 917774,\n",
       " 1048843,\n",
       " 360720,\n",
       " 246032,\n",
       " 934160,\n",
       " 639251,\n",
       " 721172,\n",
       " 164118,\n",
       " 950550,\n",
       " 1245463,\n",
       " 508182,\n",
       " 327962,\n",
       " 672024,\n",
       " 311581,\n",
       " 737567,\n",
       " 950560,\n",
       " 114981,\n",
       " 1098024,\n",
       " 737577,\n",
       " 1130792,\n",
       " 229677,\n",
       " 1163567,\n",
       " 639280,\n",
       " 491826,\n",
       " 295219,\n",
       " 98616,\n",
       " 524600,\n",
       " 639288,\n",
       " 639290,\n",
       " 917820,\n",
       " 131389,\n",
       " 82235,\n",
       " 1245505,\n",
       " 524612,\n",
       " 442699,\n",
       " 672076,\n",
       " 868683,\n",
       " 655694,\n",
       " 950606,\n",
       " 1261900,\n",
       " 246097,\n",
       " 213334,\n",
       " 967002,\n",
       " 475484,\n",
       " 426333,\n",
       " 115037,\n",
       " 82278,\n",
       " 82279,\n",
       " 147816,\n",
       " 934251,\n",
       " 98668,\n",
       " 999789,\n",
       " 508273,\n",
       " 1130867,\n",
       " 622967,\n",
       " 786809,\n",
       " 999801,\n",
       " 917883,\n",
       " 1130874,\n",
       " 115071,\n",
       " 65921,\n",
       " 147845,\n",
       " 622982,\n",
       " 147848,\n",
       " 426377,\n",
       " 737674,\n",
       " 737675,\n",
       " 197004,\n",
       " 328077,\n",
       " 131470,\n",
       " 295310,\n",
       " 328084,\n",
       " 1130902,\n",
       " 1261979,\n",
       " 606620,\n",
       " 508321,\n",
       " 98722,\n",
       " 852386,\n",
       " 98725,\n",
       " 819622,\n",
       " 115109,\n",
       " 344488,\n",
       " 115113,\n",
       " 295338,\n",
       " 917930,\n",
       " 786860,\n",
       " 328109,\n",
       " 1245614,\n",
       " 1196456,\n",
       " 164272,\n",
       " 868790,\n",
       " 344504,\n",
       " 377276,\n",
       " 770493,\n",
       " 868796,\n",
       " 491969,\n",
       " 197058,\n",
       " 934339,\n",
       " 475589,\n",
       " 393670,\n",
       " 16846,\n",
       " 1032654,\n",
       " 131536,\n",
       " 737746,\n",
       " 1098195,\n",
       " 459220,\n",
       " 66005,\n",
       " 410070,\n",
       " 311767,\n",
       " 1245659,\n",
       " 541152,\n",
       " 672224,\n",
       " 786921,\n",
       " 442859,\n",
       " 590318,\n",
       " 524783,\n",
       " 885235,\n",
       " 360948,\n",
       " 852469,\n",
       " 983539,\n",
       " 1081844,\n",
       " 770548,\n",
       " 606711,\n",
       " 557562,\n",
       " 393724,\n",
       " 557566,\n",
       " 836094,\n",
       " 1081857,\n",
       " 426498,\n",
       " 98819,\n",
       " 983557,\n",
       " 557574,\n",
       " 967173,\n",
       " 623113,\n",
       " 197130,\n",
       " 754185,\n",
       " 360972,\n",
       " 1049099,\n",
       " 16910,\n",
       " 1114640,\n",
       " 590356,\n",
       " 147989,\n",
       " 1098261,\n",
       " 1131029,\n",
       " 1163798,\n",
       " 279066,\n",
       " 1262107,\n",
       " 1212958,\n",
       " 754208,\n",
       " 1114659,\n",
       " 16933,\n",
       " 655911,\n",
       " 279088,\n",
       " 1081905,\n",
       " 475696,\n",
       " 49715,\n",
       " 1278516,\n",
       " 115253,\n",
       " 344631,\n",
       " 426553,\n",
       " 213561,\n",
       " 311868,\n",
       " 66117,\n",
       " 344645,\n",
       " 295497,\n",
       " 475721,\n",
       " 1213003,\n",
       " 475723,\n",
       " 1032779,\n",
       " 868944,\n",
       " 901714,\n",
       " 475731,\n",
       " 164436,\n",
       " 475732,\n",
       " 754263,\n",
       " 524888,\n",
       " 475735,\n",
       " 475738,\n",
       " 524892,\n",
       " 426589,\n",
       " 66142,\n",
       " 1131105,\n",
       " 98914,\n",
       " 1098338,\n",
       " 213611,\n",
       " 868971,\n",
       " 901739,\n",
       " 787059,\n",
       " 1000052,\n",
       " 524917,\n",
       " 115317,\n",
       " 213621,\n",
       " 606839,\n",
       " 754297,\n",
       " 721534,\n",
       " 1131135,\n",
       " 1131140,\n",
       " 836229,\n",
       " 1163913,\n",
       " 311947,\n",
       " 1163916,\n",
       " 82573,\n",
       " 443022,\n",
       " 131727,\n",
       " 869007,\n",
       " 1163920,\n",
       " 574098,\n",
       " 606866,\n",
       " 574100,\n",
       " 197269,\n",
       " 426646,\n",
       " 557718,\n",
       " 885400,\n",
       " 983701,\n",
       " 623258,\n",
       " 721563,\n",
       " 623260,\n",
       " 213653,\n",
       " 770709,\n",
       " 1163935,\n",
       " 656032,\n",
       " 1229477,\n",
       " 787111,\n",
       " 901802,\n",
       " 885420,\n",
       " 459439,\n",
       " 246450,\n",
       " 852659,\n",
       " 950965,\n",
       " 361142,\n",
       " 1016504,\n",
       " 262841,\n",
       " 705209,\n",
       " 508603,\n",
       " 1131197,\n",
       " 99013,\n",
       " 49868,\n",
       " 869069,\n",
       " 459473,\n",
       " 787154,\n",
       " 361171,\n",
       " 688856,\n",
       " 885465,\n",
       " 262875,\n",
       " 1082076,\n",
       " 262877,\n",
       " 197342,\n",
       " 590558,\n",
       " 1032925,\n",
       " 262888,\n",
       " 262889,\n",
       " 262892,\n",
       " 262893,\n",
       " 262894,\n",
       " 410355,\n",
       " 99060,\n",
       " 1131255,\n",
       " 262904,\n",
       " 393977,\n",
       " 262907,\n",
       " 262908,\n",
       " 393981,\n",
       " 262910,\n",
       " 262911,\n",
       " 262912,\n",
       " 262913,\n",
       " 918268,\n",
       " 1180414,\n",
       " 951045,\n",
       " 459530,\n",
       " 590602,\n",
       " 279309,\n",
       " 951059,\n",
       " 312083,\n",
       " 99094,\n",
       " 1196823,\n",
       " 230168,\n",
       " 901915,\n",
       " 1164060,\n",
       " 623389,\n",
       " 426784,\n",
       " 672544,\n",
       " 279329,\n",
       " 279331,\n",
       " 361255,\n",
       " 377639,\n",
       " 361258,\n",
       " 131883,\n",
       " 246571,\n",
       " 1065772,\n",
       " 492335,\n",
       " 885551,\n",
       " 279343,\n",
       " 49974,\n",
       " 705334,\n",
       " 721725,\n",
       " 279359,\n",
       " 213825,\n",
       " 246593,\n",
       " 574274,\n",
       " 361284,\n",
       " 836421,\n",
       " 754504,\n",
       " 836424,\n",
       " 1033036,\n",
       " 1229652,\n",
       " 459607,\n",
       " 1180504,\n",
       " 557913,\n",
       " 164702,\n",
       " 99167,\n",
       " 607074,\n",
       " 410468,\n",
       " 164711,\n",
       " 590697,\n",
       " 377705,\n",
       " 574314,\n",
       " 918382,\n",
       " 869231,\n",
       " 377715,\n",
       " 410487,\n",
       " 902009,\n",
       " 1180539,\n",
       " 1164155,\n",
       " 639874,\n",
       " 918404,\n",
       " 443269,\n",
       " 295816,\n",
       " 770956,\n",
       " 66446,\n",
       " 230287,\n",
       " 915,\n",
       " 1000340,\n",
       " 721815,\n",
       " 17303,\n",
       " 1262487,\n",
       " 574363,\n",
       " 623518,\n",
       " 869282,\n",
       " 344995,\n",
       " 836516,\n",
       " 213925,\n",
       " 705445,\n",
       " 1246124,\n",
       " 230318,\n",
       " 246704,\n",
       " 525233,\n",
       " 279473,\n",
       " 967600,\n",
       " 1164212,\n",
       " 33727,\n",
       " 148416,\n",
       " 951233,\n",
       " 689090,\n",
       " 836546,\n",
       " 263108,\n",
       " 1098693,\n",
       " 328648,\n",
       " 721864,\n",
       " 492492,\n",
       " 66509,\n",
       " 803789,\n",
       " 361424,\n",
       " 639954,\n",
       " 738258,\n",
       " 590806,\n",
       " 986,\n",
       " 115674,\n",
       " 377820,\n",
       " 574431,\n",
       " 1147873,\n",
       " 377825,\n",
       " 1262563,\n",
       " 476133,\n",
       " 1164261,\n",
       " 1065959,\n",
       " 1197030,\n",
       " 689129,\n",
       " 1081339,\n",
       " 181230,\n",
       " 312304,\n",
       " 148465,\n",
       " 590835,\n",
       " 312310,\n",
       " 345080,\n",
       " 410621,\n",
       " 672768,\n",
       " 705538,\n",
       " 902147,\n",
       " 181252,\n",
       " 590854,\n",
       " 689159,\n",
       " 312332,\n",
       " 869395,\n",
       " 1082388,\n",
       " 902166,\n",
       " 738332,\n",
       " 836639,\n",
       " 1164321,\n",
       " 558117,\n",
       " 1115175,\n",
       " 885800,\n",
       " 951340,\n",
       " 394286,\n",
       " 771121,\n",
       " 1115188,\n",
       " 279605,\n",
       " 328762,\n",
       " 230459,\n",
       " 705596,\n",
       " 705597,\n",
       " 836672,\n",
       " 99396,\n",
       " 99400,\n",
       " 787528,\n",
       " 1279050,\n",
       " 541769,\n",
       " 1082444,\n",
       " 803921,\n",
       " 132179,\n",
       " 1000541,\n",
       " 689247,\n",
       " 738399,\n",
       " 115813,\n",
       " 607336,\n",
       " 1229933,\n",
       " 296046,\n",
       " 214130,\n",
       " 1066102,\n",
       " 214135,\n",
       " 115832,\n",
       " 115833,\n",
       " 1066106,\n",
       " 1164409,\n",
       " 164988,\n",
       " 1164411,\n",
       " 345214,\n",
       " 1197182,\n",
       " 83073,\n",
       " 459911,\n",
       " 509074,\n",
       " 623764,\n",
       " 541851,\n",
       " 853148,\n",
       " 328864,\n",
       " 1066154,\n",
       " 83117,\n",
       " 705711,\n",
       " 541872,\n",
       " 836784,\n",
       " 918707,\n",
       " 476341,\n",
       " 1115318,\n",
       " 345271,\n",
       " 459962,\n",
       " 918717,\n",
       " 689345,\n",
       " 1000648,\n",
       " 902347,\n",
       " 1164492,\n",
       " 328909,\n",
       " 574670,\n",
       " 771278,\n",
       " 1033424,\n",
       " 312530,\n",
       " 427219,\n",
       " 378067,\n",
       " 263381,\n",
       " 1197272,\n",
       " 66777,\n",
       " 525529,\n",
       " 312542,\n",
       " 99551,\n",
       " 50404,\n",
       " 214244,\n",
       " 722153,\n",
       " 722154,\n",
       " 902377,\n",
       " 378090,\n",
       " 214252,\n",
       " 607470,\n",
       " 296177,\n",
       " 1197298,\n",
       " 34037,\n",
       " 476406,\n",
       " 296184,\n",
       " 181497,\n",
       " 886016,\n",
       " 1049856,\n",
       " 738562,\n",
       " 99587,\n",
       " 722181,\n",
       " 902407,\n",
       " 984330,\n",
       " 214282,\n",
       " 279819,\n",
       " 738573,\n",
       " 984334,\n",
       " 754960,\n",
       " 558360,\n",
       " 181533,\n",
       " 509213,\n",
       " 460063,\n",
       " 591138,\n",
       " 1180962,\n",
       " 328998,\n",
       " 607528,\n",
       " 214315,\n",
       " 83244,\n",
       " 312619,\n",
       " 492851,\n",
       " 378169,\n",
       " 492858,\n",
       " 197950,\n",
       " 1066305,\n",
       " 99651,\n",
       " 820548,\n",
       " 820549,\n",
       " 34121,\n",
       " 591181,\n",
       " 656719,\n",
       " 345426,\n",
       " 1099092,\n",
       " 492886,\n",
       " 197976,\n",
       " 247128,\n",
       " 165210,\n",
       " 623963,\n",
       " 230748,\n",
       " 525660,\n",
       " 443741,\n",
       " 66911,\n",
       " 476512,\n",
       " 378211,\n",
       " 345444,\n",
       " 165225,\n",
       " 673129,\n",
       " 460139,\n",
       " 591214,\n",
       " 198001,\n",
       " 329075,\n",
       " 132469,\n",
       " 247158,\n",
       " 1213815,\n",
       " 1131897,\n",
       " 66938,\n",
       " 394618,\n",
       " 99709,\n",
       " 525694,\n",
       " 836992,\n",
       " 329090,\n",
       " 116101,\n",
       " 443786,\n",
       " 116109,\n",
       " 198030,\n",
       " 935309,\n",
       " 427408,\n",
       " 1230222,\n",
       " 1262992,\n",
       " 1197457,\n",
       " 411026,\n",
       " 34198,\n",
       " 968086,\n",
       " 247192,\n",
       " 1230235,\n",
       " 1050013,\n",
       " 99742,\n",
       " 853410,\n",
       " 378276,\n",
       " 312742,\n",
       " 984488,\n",
       " 1246633,\n",
       " 329132,\n",
       " 1033646,\n",
       " 378287,\n",
       " 591281,\n",
       " 361911,\n",
       " 853432,\n",
       " 329145,\n",
       " 1467,\n",
       " 1000891,\n",
       " 574909,\n",
       " 230847,\n",
       " 1017280,\n",
       " 1131969,\n",
       " 296389,\n",
       " 493000,\n",
       " 804296,\n",
       " 460234,\n",
       " 1131978,\n",
       " 361935,\n",
       " 181712,\n",
       " 280017,\n",
       " 443862,\n",
       " 918999,\n",
       " 1246681,\n",
       " 624095,\n",
       " 574948,\n",
       " 640484,\n",
       " 263654,\n",
       " 1181158,\n",
       " 542182,\n",
       " 1230311,\n",
       " 722410,\n",
       " 1263083,\n",
       " 214512,\n",
       " 787953,\n",
       " 329202,\n",
       " 1181170,\n",
       " 1263088,\n",
       " 591349,\n",
       " 1082870,\n",
       " 411122,\n",
       " 771575,\n",
       " 198137,\n",
       " 99835,\n",
       " 50684,\n",
       " 984573,\n",
       " 853502,\n",
       " 591359,\n",
       " 689663,\n",
       " 296451,\n",
       " 558595,\n",
       " 706051,\n",
       " 394760,\n",
       " 378378,\n",
       " 1099275,\n",
       " 1197579,\n",
       " 17933,\n",
       " 280083,\n",
       " 575002,\n",
       " 706074,\n",
       " 198173,\n",
       " 525854,\n",
       " 165407,\n",
       " 99873,\n",
       " 280101,\n",
       " 329254,\n",
       " 689703,\n",
       " 640550,\n",
       " 67114,\n",
       " 493100,\n",
       " 722478,\n",
       " 837167,\n",
       " 1001010,\n",
       " 788019,\n",
       " 67129,\n",
       " 345657,\n",
       " 230971,\n",
       " 1181245,\n",
       " 1279549,\n",
       " 525898,\n",
       " 17995,\n",
       " 902734,\n",
       " 935503,\n",
       " 362064,\n",
       " 1164880,\n",
       " 1164882,\n",
       " 1017427,\n",
       " 1164892,\n",
       " 443999,\n",
       " 1148512,\n",
       " 1164898,\n",
       " 132708,\n",
       " 231012,\n",
       " 132710,\n",
       " 427623,\n",
       " 312934,\n",
       " 165481,\n",
       " 1230446,\n",
       " 411247,\n",
       " 968305,\n",
       " 214643,\n",
       " 673399,\n",
       " 329336,\n",
       " 444025,\n",
       " 804473,\n",
       " 755324,\n",
       " 460416,\n",
       " 755332,\n",
       " 1670,\n",
       " 99979,\n",
       " 673419,\n",
       " 1181328,\n",
       " 1066640,\n",
       " 116371,\n",
       " 525972,\n",
       " 444054,\n",
       " 558743,\n",
       " 1132183,\n",
       " 1001112,\n",
       " 673436,\n",
       " 149149,\n",
       " 345761,\n",
       " 345762,\n",
       " 542377,\n",
       " 509613,\n",
       " 116399,\n",
       " 247471,\n",
       " 198322,\n",
       " 738996,\n",
       " 1263286,\n",
       " 149179,\n",
       " 214715,\n",
       " 951997,\n",
       " 804541,\n",
       " 526015,\n",
       " 313024,\n",
       " 18114,\n",
       " 706242,\n",
       " 1164996,\n",
       " 1001158,\n",
       " 100039,\n",
       " 50887,\n",
       " 1263304,\n",
       " 1017546,\n",
       " 837323,\n",
       " 607952,\n",
       " 870099,\n",
       " 296661,\n",
       " 853719,\n",
       " 181977,\n",
       " 919262,\n",
       " 1263327,\n",
       " 509664,\n",
       " 214759,\n",
       " 1165031,\n",
       " 231145,\n",
       " 575211,\n",
       " 919277,\n",
       " 50925,\n",
       " 100082,\n",
       " 476917,\n",
       " 493303,\n",
       " 67320,\n",
       " 1001208,\n",
       " 526075,\n",
       " 395004,\n",
       " 575228,\n",
       " 706301,\n",
       " 280320,\n",
       " 395011,\n",
       " 395012,\n",
       " 296712,\n",
       " 100105,\n",
       " 395016,\n",
       " 427786,\n",
       " 1017611,\n",
       " 345866,\n",
       " 509712,\n",
       " 18197,\n",
       " 263959,\n",
       " 280345,\n",
       " 1247002,\n",
       " 476958,\n",
       " 886559,\n",
       " 640798,\n",
       " 624421,\n",
       " 296743,\n",
       " 1197867,\n",
       " 558893,\n",
       " 1165103,\n",
       " 689969,\n",
       " 296754,\n",
       " 984881,\n",
       " 411442,\n",
       " 378681,\n",
       " 378685,\n",
       " 444222,\n",
       " 378688,\n",
       " 378690,\n",
       " 395077,\n",
       " 771911,\n",
       " 1050440,\n",
       " 444235,\n",
       " 591692,\n",
       " 1050444,\n",
       " 395086,\n",
       " 411469,\n",
       " 608080,\n",
       " 952146,\n",
       " 640852,\n",
       " 853845,\n",
       " 1066838,\n",
       " 1165142,\n",
       " 968536,\n",
       " 427869,\n",
       " 329566,\n",
       " 149341,\n",
       " 165735,\n",
       " 1050471,\n",
       " 1050474,\n",
       " 1050475,\n",
       " 296812,\n",
       " 214897,\n",
       " 1050482,\n",
       " 509809,\n",
       " 18291,\n",
       " 1132405,\n",
       " 345974,\n",
       " 542582,\n",
       " 755577,\n",
       " 673659,\n",
       " 214911,\n",
       " 575367,\n",
       " 182152,\n",
       " 1132423,\n",
       " 280460,\n",
       " 18319,\n",
       " 165780,\n",
       " 706452,\n",
       " 18329,\n",
       " 34714,\n",
       " 34715,\n",
       " 591772,\n",
       " 755614,\n",
       " 591775,\n",
       " 231328,\n",
       " 1034144,\n",
       " 313251,\n",
       " 395173,\n",
       " 804778,\n",
       " 460719,\n",
       " 116655,\n",
       " 788402,\n",
       " 853942,\n",
       " 133047,\n",
       " 1116086,\n",
       " 575418,\n",
       " 706491,\n",
       " 985020,\n",
       " 100291,\n",
       " 755652,\n",
       " 67525,\n",
       " 100293,\n",
       " 1001412,\n",
       " 182213,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_candidate_resources('sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TF-IDF\n",
    "#### ğ‘‡ğ¹(ğ‘¤, ğ‘‘) = ğ‘“ğ‘Ÿğ‘’ğ‘(ğ‘¤, ğ‘‘) Ã· (ğ‘šğ‘ğ‘¥_ğ‘‘)\n",
    "#### ğ¼ğ·ğ¹(ğ‘¤) = ğ‘™ğ‘œğ‘”__2 (ğ‘ Ã· ğ‘›_ğ‘¤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(wiki_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def tf_idf(split_query, document_id):\n",
    "    score = 0.001 # not zero for normalization\n",
    "\n",
    "    if document_id == 0:\n",
    "        print(\"error in tf-idf function\")\n",
    "        return\n",
    "\n",
    "    for term in split_query:\n",
    "        if term not in inv_idx:\n",
    "            continue\n",
    "        score += (inv_idx[term][document_id] / wiki_dataframe['most_frequent_term'][document_id - 1][0][1]) * math.log((len(wiki_dataframe) / len(inv_idx[term])), 2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking candidate resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidate_resources(query, candidate_resources):\n",
    "  ranked_candidates = {}\n",
    "  for document_id in candidate_resources:\n",
    "    ranked_candidates[document_id] = tf_idf(query.split(), document_id)\n",
    "  return sorted(ranked_candidates.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721020, 7.318117658755251)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_and_rank_candidate_resources(query):\n",
    "    candidates = identify_candidate_resources(query)\n",
    "    return rank_candidate_resources(query, candidates)\n",
    "relevant_resources = find_and_rank_candidate_resources(\"sex\")[0]\n",
    "relevant_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippet(query, document_id):\n",
    "    row = wiki_dataframe[wiki_dataframe['id'] == document_id + 1]\n",
    "    snippet = (  # tuple in the form (title, sentences)\n",
    "        row[\"title\"].to_string(index=False),\n",
    "        generate_sentence_snippets(query, document_id, int(row[\"title\"].str.len())))\n",
    "    return snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Moroccoâ€“Saudi Arabia relations'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataframe[\"title\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_sentence_snippets(query, document_id, len_title):\n",
    "    pattern = '(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s'\n",
    "    top_two = []  # format is (sentence, cosine_similarity_score)\n",
    "    vectorized_query = vectorize(query, document_id)\n",
    "    for sentence in re.split(pattern, wiki_dataframe[\"content\"][document_id][len_title:]):\n",
    "        sentence = sentence.replace(\"\\r\", \"\").replace(\"\\n\", \"\")  # 4 is for removal of \\r\\n\\r\\n for each sentence.\n",
    "        lsentence = sentence.lower()\n",
    "\n",
    "        sentence_score = cosine_similarity(vectorized_query, vectorize(lsentence, document_id))\n",
    "        if len(top_two) < 2:\n",
    "            top_two.append((sentence, sentence_score))\n",
    "            top_two.sort(key=lambda item: item[1], reverse=True)\n",
    "            continue\n",
    "\n",
    "        for index, entry in enumerate(top_two):  # this loop should only run twice\n",
    "            if sentence_score > entry[1]:\n",
    "                top_two[index] = (sentence, sentence_score)\n",
    "                top_two.sort(key=lambda item: item[1], reverse=True)\n",
    "                break\n",
    "    if len(top_two) < 2:\n",
    "        result = top_two[0][0]\n",
    "    else:\n",
    "        result = (top_two[0][0], top_two[1][0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(phrase, document_id):\n",
    "    arr = []\n",
    "    for word in phrase.split():\n",
    "        arr.append(tf_idf([word], document_id))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def cosine_similarity(vectorized_query, vectorized_sentence):\n",
    "    lenf = max(len(vectorized_query), len(vectorized_sentence))\n",
    "    for i in range (lenf + 1):\n",
    "        if len(vectorized_query) == len(vectorized_sentence):\n",
    "            break\n",
    "        if len(vectorized_query) < i:\n",
    "            vectorized_query.append(0.001)\n",
    "        if len(vectorized_sentence) < i:\n",
    "            vectorized_sentence.append(0.001)\n",
    "    return 1 - spatial.distance.cosine(list(vectorized_sentence), list(vectorized_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "def load_files():\n",
    "    with open(\"./data/wiki_dataframe_augmented_nltk_corpus_to_remove_non-english.pkl\", \"rb\") as wiki_df:\n",
    "        wiki_dataframe = pickle.load(wiki_df)\n",
    "    inv_idx = pickle.load(open(\"./data/inv_idx_augmented_nltk_corpus_to_remove_non-english.pkl\", \"rb\"))\n",
    "    return wiki_dataframe, inv_idx\n",
    "wiki_dataframe, inv_idx = load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying Candidate Queries...\n",
      "aol query                                    amazon\n",
      "                    academy award nominee\n",
      "                     family tree template\n",
      "                free family tree template\n",
      "                        free family chart\n",
      "               free blank geneology chart\n",
      "                                     file\n",
      "                           metro district\n",
      "                     denver county police\n",
      "                           metro district\n",
      "                    nike woman sportswear\n",
      "                                     nike\n",
      "                                    woman\n",
      "                    woman track suit nike\n",
      "                                    woman\n",
      "                      woman athletic wear\n",
      "                               gart sport\n",
      "                     nike fitness apparel\n",
      "                  steamboat spring resort\n",
      "                 parking steamboat spring\n",
      "                                   goggle\n",
      "                         galapagos iguana\n",
      "                         galapagos iguana\n",
      "                        galapagos penguin\n",
      "                        galapagos penguin\n",
      "        celebrity resort steamboat spring\n",
      "celebrity resort steamboat spring hilltop\n",
      "                                   denver\n",
      "                           apple computer\n",
      "                     philip pullman award\n",
      "                                   google\n",
      "                        blue sky lacrosse\n",
      "                        string yarn store\n",
      "                  sting knitting boutique\n",
      "                         essential tremor\n",
      "                             postage rate\n",
      "                      hamilton new jersey\n",
      "             hamilton new jersey lacrosse\n",
      "                hamilton new jersey hotel\n",
      "                            lacrosse rule\n",
      "                        boy lacrosse rule\n",
      "            boy lacrosse stick regulation\n",
      "                men lacrosse stick rating\n",
      "                 comparing lacrosse stick\n",
      "          comparing lacrosse stick pocket\n",
      "         men lacrosse stick pocket review\n",
      "                  best lacrosse stick boy\n",
      "aol query                 fidelity\n",
      "                fidelity\n",
      "                   woman\n",
      "             woman dress\n",
      "                  penney\n",
      "                  google\n",
      "            google earth\n",
      "              cruise cam\n",
      "              jewely box\n",
      " necklace earings hanger\n",
      "internal revenue service\n",
      "                mirepoix\n",
      "      bay bridge web cam\n",
      "                  kaiser\n",
      "     cruise ship illness\n",
      "                  kaiser\n",
      "               iris bulb\n",
      "          swim cover ups\n",
      "                 bermuda\n",
      "              u passport\n",
      "              u passport\n",
      "             craigs list\n",
      "              head start\n",
      "             baywood inn\n",
      "         oakland airport\n",
      "    bart oakland airport\n",
      "aol query                concert\n",
      "         music concert\n",
      "         music concert\n",
      "         music concert\n",
      "                dating\n",
      "                 woman\n",
      "              slipknot\n",
      "           beauty geek\n",
      "             chat line\n",
      "                dating\n",
      "          dating local\n",
      "             chat line\n",
      "                dating\n",
      "                dating\n",
      "             chat line\n",
      "                 woman\n",
      "      heavy metel iowa\n",
      "                 yahoo\n",
      "   adult friend finder\n",
      "     tax refund status\n",
      "                 yahoo\n",
      "      heavy metel iowa\n",
      "                 trapt\n",
      "           dirty woman\n",
      "           adult porno\n",
      "           adult porno\n",
      "           adult porno\n",
      "            meet woman\n",
      "sexy single woman iowa\n",
      "           fear factor\n",
      "  sexual friend finder\n",
      "sexy single woman iowa\n",
      "             disturbed\n",
      "           price right\n",
      "                u bank\n",
      "           men fitness\n",
      "   adult friend finder\n",
      "            chat woman\n",
      "            bryan adam\n",
      "             love song\n",
      "             love song\n",
      "            chat woman\n",
      "             love song\n",
      "            chat woman\n",
      "           storm watch\n",
      "      storm watch iowa\n",
      "             love song\n",
      "                  chat\n",
      "    naked single woman\n",
      "       naked celebrity\n",
      "             tommy lee\n",
      "     naked woman video\n",
      "     naked woman video\n",
      "     naked woman video\n",
      "      avenge sevenfold\n",
      "         rolling stone\n",
      "         rolling stone\n",
      "         rolling stone\n",
      "           screen name\n",
      "                 movie\n",
      "aol query woman\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22404/3016524865.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0msearch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"woman\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22404/3016524865.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#     title, sentences = get_snippet(query, resource[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#     results[resource[0]] = [title, sentences]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mranked_candidate_queries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_rank_candidate_queries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"query_suggestions\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mquery_suggestion\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mranked_candidate_queries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22404/2113190807.py\u001b[0m in \u001b[0;36mfind_rank_candidate_queries\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_rank_candidate_queries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcandidate_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midentify_candidate_queries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrank_candidate_queries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22404/3277356836.py\u001b[0m in \u001b[0;36midentify_candidate_queries\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maol_query\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Processed Query'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0maol_query\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Processed Query'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                 \u001b[0mcandidate_queries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maol_query\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcandidate_queries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'DataFrame'"
     ]
    }
   ],
   "source": [
    "def search(query):\n",
    "    # print(\"Generating results...\")\n",
    "    # ranked_candidate_resources = find_and_rank_candidate_resources(query)[0:10]\n",
    "    results = {}\n",
    "    # for resource in ranked_candidate_resources:\n",
    "    #     print(\"resource\", resource, file=sys.stderr)\n",
    "    #     title, sentences = get_snippet(query, resource[0])\n",
    "    #     results[resource[0]] = [title, sentences]\n",
    "    ranked_candidate_queries = find_rank_candidate_queries(query)\n",
    "    results[\"query_suggestions\"] = []\n",
    "    for query_suggestion in ranked_candidate_queries[:5]:\n",
    "        results[\"query_suggestions\"].append(query_suggestion)\n",
    "\n",
    "    return results\n",
    "\n",
    "search_results = search(\"woman\")\n",
    "print(\"results:\")\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonID                         3352559\n",
       "Query                        actor ken\n",
       "QueryTime          2006-03-01 17:42:50\n",
       "Processed Query              actor ken\n",
       "Name: 1065383, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aol_query_log = pickle.load(open(\"./data/aol_query_log_data.pkl\", \"rb\"))\n",
    "\n",
    "aol_query_log.iloc[1065383]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa55fa09e0e4422978c3061674d30740611d5f2a5ecbc5414dc2c8743e27cab3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
